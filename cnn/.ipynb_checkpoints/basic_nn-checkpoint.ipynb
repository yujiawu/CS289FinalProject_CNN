{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import pylab\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "image_by_pixel = sio.loadmat('S0D2_jpgimage.mat')\n",
    "label_by_pixel = sio.loadmat('S0D2_jpglabel.mat')\n",
    "\n",
    "test_image_by_pixel = sio.loadmat('S1D1_jpgimage.mat')\n",
    "test_label_by_pixel = sio.loadmat('S1D1_jpglabel.mat')\n",
    "#test_image_by_pixel = sio.loadmat('s3d1_jpgimage.mat')\n",
    "# test_label_by_pixel = sio.loadmat('s3d1_jpglabel.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4096)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def image_process(image, label, down_sample_rate=4):\n",
    "    images = image['allimages'][::down_sample_rate ,::down_sample_rate , :]  # downsample the image\n",
    "    labels = label['disc'][::down_sample_rate ,::down_sample_rate , :]       # downsample the pixel\n",
    "    image_length1 = images.shape[0]\n",
    "    image_length2 = images.shape[1]\n",
    "    num_image = images.shape[2]                # number of images\n",
    "    image_size = image_length1*image_length2   # image size \n",
    "    label_whole = np.zeros((num_image, output_size))#the label of whether the image is all trash or not\n",
    "    images_processed = np.zeros((num_image, image_size))    \n",
    "    for _ in range(num_image):\n",
    "        label_whole[_] = 1*(np.linalg.norm(labels[:,:,_]) != 0) \n",
    "        images_processed[_] = images[:,:,_].reshape(image_size)\n",
    "    return images_processed, label_whole, image_size\n",
    "\n",
    "\n",
    "output_size = 1                            # output portal size 1: 0 - garbge 1 - useful\n",
    "\n",
    "images_processed, label_whole, image_size = image_process(image_by_pixel, label_by_pixel, 4)\n",
    "test_images_processed, test_label_whole, _ =  image_process(test_image_by_pixel, test_label_by_pixel, 4)\n",
    "\n",
    "images_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# image_processed.shape\n",
    "# label_whole\n",
    "# image_size\n",
    "# test_image_processed.shape\n",
    "# test_label_whole.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define variable placeholder for tf\n",
    "x = tf.placeholder('float',[None, image_size])\n",
    "y = tf.placeholder('float', [None, output_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "n_nodes_hl3 = 500\n",
    "def neural_net_for_whole(data):\n",
    "    hidden_1_layer = {'weights':tf.Variable(tf.random_normal([image_size, n_nodes_hl1])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "    hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "    hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "\n",
    "    output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, output_size])),\n",
    "                    'biases':tf.Variable(tf.random_normal([output_size]))}\n",
    "    \n",
    "    \n",
    "    l1 = tf.add(tf.matmul(data,hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "\n",
    "    l3 = tf.add(tf.matmul(l2,hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "\n",
    "    output = tf.matmul(l3,output_layer['weights']) + output_layer['biases']\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_neural_network(x):\n",
    "    prediction = neural_net_for_whole(x)\n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=prediction, labels=y) )\n",
    "    # double check what cost function to use\n",
    "    # cost = tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y) \n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "    hm_epochs = 150 # number of epochs\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(hm_epochs):\n",
    "            _, epoch_loss = sess.run([optimizer, cost], feed_dict={x: images_processed, y: label_whole})\n",
    "            print('Epoch', epoch+1, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "        \n",
    "        print(prediction.eval({x:images_processed}), label_whole)   \n",
    "        #correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "        #accuracy = tf.reduce_mean(tf.cast(correct, 'float'))            \n",
    "        #print('Accuracy:',accuracy.eval({x:test_images_processed, y:test_label_whole}))\n",
    "        #print(correct.eval({x:test_images_processed, y:test_label_whole}))\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed out of 150 loss: 1.36718e+06\n",
      "Epoch 2 completed out of 150 loss: 143994.0\n",
      "Epoch 3 completed out of 150 loss: 2.05602e+06\n",
      "Epoch 4 completed out of 150 loss: 205090.0\n",
      "Epoch 5 completed out of 150 loss: 40616.8\n",
      "Epoch 6 completed out of 150 loss: 104908.0\n",
      "Epoch 7 completed out of 150 loss: 175341.0\n",
      "Epoch 8 completed out of 150 loss: 217137.0\n",
      "Epoch 9 completed out of 150 loss: 243580.0\n",
      "Epoch 10 completed out of 150 loss: 232367.0\n",
      "Epoch 11 completed out of 150 loss: 191175.0\n",
      "Epoch 12 completed out of 150 loss: 146251.0\n",
      "Epoch 13 completed out of 150 loss: 98351.4\n",
      "Epoch 14 completed out of 150 loss: 58372.4\n",
      "Epoch 15 completed out of 150 loss: 38896.3\n",
      "Epoch 16 completed out of 150 loss: 18775.0\n",
      "Epoch 17 completed out of 150 loss: 0.0\n",
      "Epoch 18 completed out of 150 loss: 6056.12\n",
      "Epoch 19 completed out of 150 loss: 10081.5\n",
      "Epoch 20 completed out of 150 loss: 9785.54\n",
      "Epoch 21 completed out of 150 loss: 5691.13\n",
      "Epoch 22 completed out of 150 loss: 0.0\n",
      "Epoch 23 completed out of 150 loss: 0.0\n",
      "Epoch 24 completed out of 150 loss: 0.0\n",
      "Epoch 25 completed out of 150 loss: 0.0\n",
      "Epoch 26 completed out of 150 loss: 1055.26\n",
      "Epoch 27 completed out of 150 loss: 0.0\n",
      "Epoch 28 completed out of 150 loss: 0.0\n",
      "Epoch 29 completed out of 150 loss: 0.0\n",
      "Epoch 30 completed out of 150 loss: 0.0\n",
      "Epoch 31 completed out of 150 loss: 0.0\n",
      "Epoch 32 completed out of 150 loss: 0.0\n",
      "Epoch 33 completed out of 150 loss: 0.0\n",
      "Epoch 34 completed out of 150 loss: 0.0\n",
      "Epoch 35 completed out of 150 loss: 0.0\n",
      "Epoch 36 completed out of 150 loss: 0.0\n",
      "Epoch 37 completed out of 150 loss: 0.0\n",
      "Epoch 38 completed out of 150 loss: 0.0\n",
      "Epoch 39 completed out of 150 loss: 0.0\n",
      "Epoch 40 completed out of 150 loss: 0.0\n",
      "Epoch 41 completed out of 150 loss: 0.0\n",
      "Epoch 42 completed out of 150 loss: 0.0\n",
      "Epoch 43 completed out of 150 loss: 0.0\n",
      "Epoch 44 completed out of 150 loss: 0.0\n",
      "Epoch 45 completed out of 150 loss: 0.0\n",
      "Epoch 46 completed out of 150 loss: 0.0\n",
      "Epoch 47 completed out of 150 loss: 0.0\n",
      "Epoch 48 completed out of 150 loss: 0.0\n",
      "Epoch 49 completed out of 150 loss: 0.0\n",
      "Epoch 50 completed out of 150 loss: 0.0\n",
      "Epoch 51 completed out of 150 loss: 0.0\n",
      "Epoch 52 completed out of 150 loss: 0.0\n",
      "Epoch 53 completed out of 150 loss: 0.0\n",
      "Epoch 54 completed out of 150 loss: 0.0\n",
      "Epoch 55 completed out of 150 loss: 0.0\n",
      "Epoch 56 completed out of 150 loss: 0.0\n",
      "Epoch 57 completed out of 150 loss: 0.0\n",
      "Epoch 58 completed out of 150 loss: 0.0\n",
      "Epoch 59 completed out of 150 loss: 0.0\n",
      "Epoch 60 completed out of 150 loss: 0.0\n",
      "Epoch 61 completed out of 150 loss: 0.0\n",
      "Epoch 62 completed out of 150 loss: 0.0\n",
      "Epoch 63 completed out of 150 loss: 0.0\n",
      "Epoch 64 completed out of 150 loss: 0.0\n",
      "Epoch 65 completed out of 150 loss: 0.0\n",
      "Epoch 66 completed out of 150 loss: 0.0\n",
      "Epoch 67 completed out of 150 loss: 0.0\n",
      "Epoch 68 completed out of 150 loss: 0.0\n",
      "Epoch 69 completed out of 150 loss: 0.0\n",
      "Epoch 70 completed out of 150 loss: 0.0\n",
      "Epoch 71 completed out of 150 loss: 0.0\n",
      "Epoch 72 completed out of 150 loss: 0.0\n",
      "Epoch 73 completed out of 150 loss: 0.0\n",
      "Epoch 74 completed out of 150 loss: 0.0\n",
      "Epoch 75 completed out of 150 loss: 0.0\n",
      "Epoch 76 completed out of 150 loss: 0.0\n",
      "Epoch 77 completed out of 150 loss: 0.0\n",
      "Epoch 78 completed out of 150 loss: 0.0\n",
      "Epoch 79 completed out of 150 loss: 0.0\n",
      "Epoch 80 completed out of 150 loss: 0.0\n",
      "Epoch 81 completed out of 150 loss: 0.0\n",
      "Epoch 82 completed out of 150 loss: 0.0\n",
      "Epoch 83 completed out of 150 loss: 0.0\n",
      "Epoch 84 completed out of 150 loss: 0.0\n",
      "Epoch 85 completed out of 150 loss: 0.0\n",
      "Epoch 86 completed out of 150 loss: 0.0\n",
      "Epoch 87 completed out of 150 loss: 0.0\n",
      "Epoch 88 completed out of 150 loss: 0.0\n",
      "Epoch 89 completed out of 150 loss: 0.0\n",
      "Epoch 90 completed out of 150 loss: 0.0\n",
      "Epoch 91 completed out of 150 loss: 0.0\n",
      "Epoch 92 completed out of 150 loss: 0.0\n",
      "Epoch 93 completed out of 150 loss: 0.0\n",
      "Epoch 94 completed out of 150 loss: 0.0\n",
      "Epoch 95 completed out of 150 loss: 0.0\n",
      "Epoch 96 completed out of 150 loss: 0.0\n",
      "Epoch 97 completed out of 150 loss: 0.0\n",
      "Epoch 98 completed out of 150 loss: 0.0\n",
      "Epoch 99 completed out of 150 loss: 0.0\n",
      "Epoch 100 completed out of 150 loss: 0.0\n",
      "Epoch 101 completed out of 150 loss: 0.0\n",
      "Epoch 102 completed out of 150 loss: 0.0\n",
      "Epoch 103 completed out of 150 loss: 0.0\n",
      "Epoch 104 completed out of 150 loss: 0.0\n",
      "Epoch 105 completed out of 150 loss: 0.0\n",
      "Epoch 106 completed out of 150 loss: 0.0\n",
      "Epoch 107 completed out of 150 loss: 0.0\n",
      "Epoch 108 completed out of 150 loss: 0.0\n",
      "Epoch 109 completed out of 150 loss: 0.0\n",
      "Epoch 110 completed out of 150 loss: 0.0\n",
      "Epoch 111 completed out of 150 loss: 0.0\n",
      "Epoch 112 completed out of 150 loss: 0.0\n",
      "Epoch 113 completed out of 150 loss: 0.0\n",
      "Epoch 114 completed out of 150 loss: 0.0\n",
      "Epoch 115 completed out of 150 loss: 0.0\n",
      "Epoch 116 completed out of 150 loss: 0.0\n",
      "Epoch 117 completed out of 150 loss: 0.0\n",
      "Epoch 118 completed out of 150 loss: 0.0\n",
      "Epoch 119 completed out of 150 loss: 0.0\n",
      "Epoch 120 completed out of 150 loss: 0.0\n",
      "Epoch 121 completed out of 150 loss: 0.0\n",
      "Epoch 122 completed out of 150 loss: 0.0\n",
      "Epoch 123 completed out of 150 loss: 0.0\n",
      "Epoch 124 completed out of 150 loss: 0.0\n",
      "Epoch 125 completed out of 150 loss: 0.0\n",
      "Epoch 126 completed out of 150 loss: 0.0\n",
      "Epoch 127 completed out of 150 loss: 0.0\n",
      "Epoch 128 completed out of 150 loss: 0.0\n",
      "Epoch 129 completed out of 150 loss: 0.0\n",
      "Epoch 130 completed out of 150 loss: 0.0\n",
      "Epoch 131 completed out of 150 loss: 0.0\n",
      "Epoch 132 completed out of 150 loss: 0.0\n",
      "Epoch 133 completed out of 150 loss: 0.0\n",
      "Epoch 134 completed out of 150 loss: 0.0\n",
      "Epoch 135 completed out of 150 loss: 0.0\n",
      "Epoch 136 completed out of 150 loss: 0.0\n",
      "Epoch 137 completed out of 150 loss: 0.0\n",
      "Epoch 138 completed out of 150 loss: 0.0\n",
      "Epoch 139 completed out of 150 loss: 0.0\n",
      "Epoch 140 completed out of 150 loss: 0.0\n",
      "Epoch 141 completed out of 150 loss: 0.0\n",
      "Epoch 142 completed out of 150 loss: 0.0\n",
      "Epoch 143 completed out of 150 loss: 0.0\n",
      "Epoch 144 completed out of 150 loss: 0.0\n",
      "Epoch 145 completed out of 150 loss: 0.0\n",
      "Epoch 146 completed out of 150 loss: 0.0\n",
      "Epoch 147 completed out of 150 loss: 0.0\n",
      "Epoch 148 completed out of 150 loss: 0.0\n",
      "Epoch 149 completed out of 150 loss: 0.0\n",
      "Epoch 150 completed out of 150 loss: 0.0\n",
      "[[ -6446039.   ]\n",
      " [ -6116528.   ]\n",
      " [ -5347542.   ]\n",
      " [-13748591.   ]\n",
      " [ -7053731.5  ]\n",
      " [ -9203726.   ]\n",
      " [-12826205.   ]\n",
      " [ -9718524.   ]\n",
      " [-10274960.   ]\n",
      " [-11364074.   ]\n",
      " [ -9197297.   ]\n",
      " [ -8370982.5  ]\n",
      " [ -8553300.   ]\n",
      " [ -4980322.5  ]\n",
      " [ -6347076.   ]\n",
      " [ -6274663.   ]\n",
      " [  4650992.   ]\n",
      " [  7785775.5  ]\n",
      " [  9760727.   ]\n",
      " [ 13790302.   ]\n",
      " [ 12563323.   ]\n",
      " [ 21390540.   ]\n",
      " [ 24980444.   ]\n",
      " [ 22979694.   ]\n",
      " [ 21403362.   ]\n",
      " [ 25833788.   ]\n",
      " [ 29122696.   ]\n",
      " [ 28491876.   ]\n",
      " [ 29680480.   ]\n",
      " [ 31145552.   ]\n",
      " [ 31667954.   ]\n",
      " [ 36820376.   ]\n",
      " [ 33314974.   ]\n",
      " [ 28760238.   ]\n",
      " [ 31364828.   ]\n",
      " [ 31087954.   ]\n",
      " [ 42624372.   ]\n",
      " [ 40482268.   ]\n",
      " [ 42551112.   ]\n",
      " [ 40572496.   ]\n",
      " [ 39297248.   ]\n",
      " [ 40679692.   ]\n",
      " [ 45145576.   ]\n",
      " [ 38440684.   ]\n",
      " [ 38423408.   ]\n",
      " [ 49553912.   ]\n",
      " [ 39874452.   ]\n",
      " [ 42334116.   ]\n",
      " [ 42276288.   ]\n",
      " [ 48010288.   ]\n",
      " [ 45742816.   ]\n",
      " [ 46070236.   ]\n",
      " [ 48908112.   ]\n",
      " [ 42083368.   ]\n",
      " [ 35692604.   ]\n",
      " [ 39957148.   ]\n",
      " [ 39270728.   ]\n",
      " [ 49558028.   ]\n",
      " [ 38163132.   ]\n",
      " [ 42796360.   ]\n",
      " [ 47120024.   ]\n",
      " [ 45153880.   ]\n",
      " [ 40381564.   ]\n",
      " [ 42784928.   ]\n",
      " [ 42806700.   ]\n",
      " [ 46385224.   ]\n",
      " [ 43817060.   ]\n",
      " [ 44098064.   ]\n",
      " [ 39261316.   ]\n",
      " [ 44591356.   ]\n",
      " [ 38956812.   ]\n",
      " [ 41702052.   ]\n",
      " [ 45848656.   ]\n",
      " [ 41554016.   ]\n",
      " [ 45510664.   ]\n",
      " [ 40792796.   ]\n",
      " [ 40643672.   ]\n",
      " [ 46442264.   ]\n",
      " [ 43014672.   ]\n",
      " [ 44837812.   ]\n",
      " [ 39331800.   ]\n",
      " [ 36765928.   ]\n",
      " [ 34387160.   ]\n",
      " [ 37674952.   ]\n",
      " [ 34050516.   ]\n",
      " [ 29175040.   ]\n",
      " [ 34654936.   ]\n",
      " [ 28556892.   ]\n",
      " [ 34207304.   ]\n",
      " [ 26286900.   ]\n",
      " [ 24840152.   ]\n",
      " [ 18834062.   ]\n",
      " [ 18797560.   ]\n",
      " [ 17275432.   ]\n",
      " [ 20094346.   ]\n",
      " [ 14351668.   ]\n",
      " [  3044144.   ]\n",
      " [  1967224.875]\n",
      " [-17949540.   ]\n",
      " [-12702060.   ]] [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
