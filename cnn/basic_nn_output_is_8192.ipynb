{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import pylab\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "image_by_pixel = sio.loadmat('combined_img.mat')\n",
    "label_by_pixel = sio.loadmat('combined_lab.mat')\n",
    "\n",
    "test_image_by_pixel = sio.loadmat('S0D2_jpgimage.mat')\n",
    "test_label_by_pixel = sio.loadmat('S0D2_jpglabel.mat')\n",
    "#test_image_by_pixel = sio.loadmat('s3d1_jpgimage.mat')\n",
    "# test_label_by_pixel = sio.loadmat('s3d1_jpglabel.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def image_process(image, label, down_sample_rate=4):\n",
    "    images = image['allimages'][::down_sample_rate ,::down_sample_rate , :]  # downsample the image\n",
    "    labels = label['disc'][::down_sample_rate ,::down_sample_rate , :]       # downsample the pixel\n",
    "    image_length1 = images.shape[0]\n",
    "    image_length2 = images.shape[1]\n",
    "    num_image = images.shape[2]                # number of images\n",
    "    image_size = image_length1*image_length2  # image size\n",
    "    label_size = image_size*2                   # label size  \n",
    "    label_whole = np.zeros((num_image, label_size))\n",
    "    # label_whole = np.zeros((num_image, output_size))#the label of whether the image is all trash or not\n",
    "    images_processed = np.zeros((num_image, image_size))    \n",
    "    for _ in range(num_image):\n",
    "        # kk= 1*(np.linalg.norm(labels[:,:,_]) != 0) \n",
    "        # if kk ==1:     \n",
    "        #    label_whole[_,1] = 1               # make output dimension = 2. for example [0,1] or [1,0]\n",
    "        # else:\n",
    "        #    label_whole[_,0] = 1\n",
    "        kk = labels[:,:,_].reshape(image_size)\n",
    "        for i in range(image_size):\n",
    "            if kk[i] == 0:\n",
    "                label_whole[_,i*2] = 1\n",
    "            else:\n",
    "                label_whole[_,i*2+1] = 1\n",
    "        images_processed[_] = images[:,:,_].reshape(image_size)\n",
    "    return images_processed, label_whole, image_size\n",
    "\n",
    "\n",
    "output_size = 4096*2                                  # output portal size 1: 0 - garbge 1 - useful\n",
    "images_processed, label_whole, image_size = image_process(image_by_pixel, label_by_pixel, 4)\n",
    "test_images_processed, test_label_whole, _ =  image_process(test_image_by_pixel, test_label_by_pixel, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#images_processed.shape\n",
    "#test_label_whole\n",
    "output_size\n",
    "#test_images_processed.shape\n",
    "#test_label_whole.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define variable placeholder for tf\n",
    "x = tf.placeholder('float',[None, image_size]) \n",
    "y = tf.placeholder('float', [None, output_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(552, 8192)\n",
      "(4096,)\n"
     ]
    }
   ],
   "source": [
    "#label_whole.shape\n",
    "print(label_whole.shape)\n",
    "print(images_processed[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_nodes_hl1 = 8000\n",
    "n_nodes_hl2 = 4000\n",
    "n_nodes_hl3 = 4000\n",
    "n_nodes_hl4 = 2000\n",
    "n_nodes_hl5 = 2000\n",
    "n_nodes_hl6 = 2000\n",
    "n_nodes_hl7 = 2000\n",
    "def neural_net_for_whole(data):\n",
    "    hidden_1_layer = {'weights':tf.Variable(tf.random_normal([image_size, n_nodes_hl1])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "    hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "    hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "    \n",
    "    #hidden_4_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_nodes_hl4])),\n",
    "    #                 'biases':tf.Variable(tf.random_normal([n_nodes_hl4]))}\n",
    "    \n",
    "    #hidden_5_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl4, n_nodes_hl5])),\n",
    "    #                 'biases':tf.Variable(tf.random_normal([n_nodes_hl5]))}\n",
    "    \n",
    "    #hidden_6_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl5, n_nodes_hl6])),\n",
    "    #                  'biases':tf.Variable(tf.random_normal([n_nodes_hl6]))}\n",
    "    \n",
    "    #hidden_7_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl6, n_nodes_hl7])),\n",
    "    #                  'biases':tf.Variable(tf.random_normal([n_nodes_hl7]))}\n",
    "\n",
    "    output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, output_size])),\n",
    "                    'biases':tf.Variable(tf.random_normal([output_size]))}\n",
    "    \n",
    "    \n",
    "    l1 = tf.add(tf.matmul(data,hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "\n",
    "    l3 = tf.add(tf.matmul(l2,hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "    \n",
    "    #l4 = tf.add(tf.matmul(l3,hidden_4_layer['weights']), hidden_4_layer['biases'])\n",
    "    #l4 = tf.nn.relu(l4)\n",
    "    \n",
    "    #l5 = tf.add(tf.matmul(l4,hidden_5_layer['weights']), hidden_5_layer['biases'])\n",
    "    #l5 = tf.nn.relu(l5)\n",
    "    \n",
    "    #l6 = tf.add(tf.matmul(l5,hidden_6_layer['weights']), hidden_6_layer['biases'])\n",
    "    #l6 = tf.nn.relu(l6)\n",
    "    \n",
    "    #l7 = tf.add(tf.matmul(l6,hidden_7_layer['weights']), hidden_7_layer['biases'])\n",
    "    #l7 = tf.nn.relu(l7)\n",
    "\n",
    "    output = tf.matmul(l3,output_layer['weights']) + output_layer['biases']\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_neural_network(x):\n",
    "    prediction = neural_net_for_whole(x)\n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=prediction, labels=y) )\n",
    "    # double check what cost function to use\n",
    "    # cost = tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y) \n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "    hm_epochs = 20 # number of epochs\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(hm_epochs):\n",
    "            _, epoch_loss = sess.run([optimizer, cost], feed_dict={x: images_processed, y: label_whole})\n",
    "            #_, epoch_loss = sess.run([optimizer, cost], feed_dict={x: aaa, y: bbb})\n",
    "            print('Epoch', epoch+1, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "        \n",
    "        lab = label_whole               # i dont know how to do this in tensorflow, the following are just python\n",
    "        pred = prediction.eval({x:images_processed})\n",
    "        print(pred.shape)\n",
    "        num_image = pred.shape[0]\n",
    "        N = int(pred.shape[1]/2)\n",
    "        pred1 = pred.reshape((num_image*N,2))\n",
    "        wrong = 0\n",
    "        right = 0\n",
    "        for i in range(num_image):\n",
    "            for j in range(N):\n",
    "                if pred[i,j*2]>pred[i,j*2+1]:\n",
    "                    if lab[i,j*2] == 1:\n",
    "                        right += 1\n",
    "                    else:\n",
    "                        wrong += 1\n",
    "                else:\n",
    "                    if lab[i,j*2] == 0:\n",
    "                        right += 1\n",
    "                    else:\n",
    "                        wrong += 1 \n",
    "        accuracy = right/(right+wrong)\n",
    "        print(accuracy)\n",
    "        print(right)\n",
    "        print(wrong)\n",
    "            \n",
    "        #print(prediction.eval({x:images_processed}), label_whole)   \n",
    "        #correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "        #correct = tf.equal(prediction, y)\n",
    "        #accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        \n",
    "        \n",
    "        #print('Accuracy:',accuracy.eval({x:images_processed, y:label_whole}))\n",
    "        #print('Accuracy:',accuracy.eval({x:test_images_processed, y:test_label_whole}))\n",
    "        #print(correct.eval({x:test_images_processed, y:test_label_whole}))\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed out of 20 loss: 1.76611e+08\n",
      "Epoch 2 completed out of 20 loss: 1.4341e+08\n",
      "Epoch 3 completed out of 20 loss: 1.16467e+08\n",
      "Epoch 4 completed out of 20 loss: 9.51831e+07\n",
      "Epoch 5 completed out of 20 loss: 7.87749e+07\n",
      "Epoch 6 completed out of 20 loss: 6.6284e+07\n",
      "Epoch 7 completed out of 20 loss: 5.67823e+07\n",
      "Epoch 8 completed out of 20 loss: 4.94595e+07\n",
      "Epoch 9 completed out of 20 loss: 4.36848e+07\n",
      "Epoch 10 completed out of 20 loss: 3.90079e+07\n",
      "Epoch 11 completed out of 20 loss: 3.51218e+07\n",
      "Epoch 12 completed out of 20 loss: 3.182e+07\n",
      "Epoch 13 completed out of 20 loss: 2.89685e+07\n",
      "Epoch 14 completed out of 20 loss: 2.64865e+07\n",
      "Epoch 15 completed out of 20 loss: 2.43205e+07\n",
      "Epoch 16 completed out of 20 loss: 2.2425e+07\n",
      "Epoch 17 completed out of 20 loss: 2.07636e+07\n",
      "Epoch 18 completed out of 20 loss: 1.92997e+07\n",
      "Epoch 19 completed out of 20 loss: 1.79979e+07\n",
      "Epoch 20 completed out of 20 loss: 1.68318e+07\n",
      "(552, 8192)\n",
      "0.7646134970844656\n",
      "1728785\n",
      "532207\n"
     ]
    }
   ],
   "source": [
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
